{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {},
   "source": [
    "# Notes Adrian\n",
    "\n",
    "\n",
    "\n",
    "- Modified the learning rate in Adam (both models), some further tuning can be done. \n",
    "- set log_every_n_steps=100\n",
    "- since the optimization here is so sensitive to initialization, i put the seed_everything(42) call also before the second model training. in case someone copies just the second model and runs it individually\n",
    "- reduced max_epochs=3000 (6k steps) to just max_steps=600\n",
    "- note on reproducibility: always run the notebook from the start for sanity. don't rerun cells with the previous model still instantiated -> Jupyter notebooks are dangerous!\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3 MAIN IDEAS FOR THE LSTM STATQUEST\n",
    "- How to use TensorBoard to see how the model traied and decide if you should try adding more epochs to training\n",
    "- How to add extra expochs without having to start over\n",
    "- How to use PyTorch LSTM class torch.nn.LSTM()\n",
    "\n",
    "# Questions\n",
    "- How to we label runs for logging and tensorboard (right now tensorboard had \"version_0\" vs \"version_1\"\n",
    "  and it would be cool if we could have it say \"homemade_lstm\" vs \"nn.LSTM()\"\n",
    "- Is there an easy way to clean up the \"lightning_logs\" (delete them etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cef3c-fd09-4f12-8248-72a2e1bc1d23",
   "metadata": {},
   "source": [
    "This is, in theory, a super simple example of how Long Short-Term Memory Neural Networks work. We'll start by implementing a single \"memory cell\" that we'll use (reusing all the weights and biases) for each element in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "First, import the modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "# from torch.optim import SGD # optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.\n",
    "from torch.optim import Adam, SGD # optim contains many optimizers. Here, we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are implementing an LSTM network by hand...\n",
    "class BasicLightningTrain(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the tensors for the LSTM\n",
    "        ##\n",
    "        ###################\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## NOTE: nn.LSTM() uses random values from a uniform distribution to initialize the tensors\n",
    "        ## Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        ## We'll start with the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        ## NOTE: We can initialize the Weights using the normal distribution (here), \n",
    "        ## or the uniform distribiton (below). In this case all Biases are initialized to 0.\n",
    "        self.stage1shortW = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage1inputW = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage1B = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.stage2shortW1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage2inputW1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage2B1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.stage2shortW2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage2inputW2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage2B2 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.stage3shortW = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage3inputW = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.stage3B = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## We can also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        ## how nn.LSTM() does it.\n",
    "#         self.stage1shortW = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage1inputW = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage1B = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.stage2shortW1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage2inputW1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage2B1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.stage2shortW2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage2inputW2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage2B2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "#         self.stage3shortW = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage3inputW = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.stage3B = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "        ## The default learning rate for Adam, the optimizer we are using instead of Stochastic Gradient Descent\n",
    "        ## is 0.001, which, in this case, will result in needing to take a relatively long time to optimize the LSTM.\n",
    "        ## The advantage, however, is that we will end up with the exact same Weights and Biases that I used\n",
    "        ## in the \"LSTMs Clearly Explained\" StatQuest, which is cool. However, we'll also show how we can\n",
    "        ## speed up training a ton by setting the learning rate to 0.1.\n",
    "        self.learning_rate = 0.001 #\n",
    "    \n",
    "        ## Lastly, for the logger, we will keep track of which output we are trying to predict\n",
    "        self.state = 0\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        ## NOTES:\n",
    "        ##  - long term memory is also called \"cell state\"\n",
    "        ##  - short term memory is also called \"hidden state\"\n",
    "        \n",
    "        ## The first stage of the LSTM determines what percentage of the old long term memory we need to retain\n",
    "        old_long_remember_percent = torch.sigmoid((short_memory * self.stage1shortW) + (input_value * self.stage1inputW) + self.stage1B)\n",
    "        \n",
    "        ## The second stage of the LSTM determines a potential memory and what percentage should be\n",
    "        ## added to the current long term memory\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.stage2shortW1) + (input_value * self.stage2inputW1) + self.stage2B1)\n",
    "        potential_memory = torch.tanh((short_memory * self.stage2shortW2) + (input_value * self.stage2inputW2) + self.stage2B2)\n",
    "        \n",
    "        ## The third, and final, stage of the LSTM determines what percentage of the long-term should be\n",
    "        ## used to create a new short term memory.\n",
    "        output_percent = torch.sigmoid((short_memory * self.stage3shortW) + (input_value * self.stage3inputW) + self.stage3B)\n",
    "        \n",
    "        long_memory = (long_memory * old_long_remember_percent) + (potential_remember_percent * potential_memory)\n",
    "        short_memory = torch.tanh(long_memory) * output_percent\n",
    "        return([long_memory, short_memory])\n",
    "        \n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        \n",
    "        ## Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        ##### The \"output value\" (or values) from an LSTM come from the last short term memory\n",
    "        return short_memory # final value for h4\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more \n",
    "                                                 # training. However, if we use the default value, we get \n",
    "                                                 # the exact same Weights and Biases that I used in\n",
    "                                                 # the LSTM Clearly Explained StatQuest video...\n",
    "        return Adam(self.parameters(), lr=self.learning_rate) \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "                \n",
    "        ## logging...\n",
    "        self.log(\"train_loss\", loss)\n",
    "        ## NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        ## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        ## the goal is to predict that the value on Day 5 = 1. We use \"self.state\" to keep track of\n",
    "        ## which company we just made a prediction for and log that output value so we can see how\n",
    "        ## well we are predicting each company's value.\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before...\n",
      "Parameters...\n",
      "stage1shortW tensor(0.3367)\n",
      "stage1inputW tensor(0.1288)\n",
      "stage1B tensor(0.)\n",
      "stage2shortW1 tensor(0.2345)\n",
      "stage2inputW1 tensor(0.2303)\n",
      "stage2B1 tensor(0.)\n",
      "stage2shortW2 tensor(-1.1229)\n",
      "stage2inputW2 tensor(-0.1863)\n",
      "stage2B2 tensor(0.)\n",
      "stage3shortW tensor(2.2082)\n",
      "stage3inputW tensor(-0.6380)\n",
      "stage3B tensor(0.)\n",
      "\n",
      "Output Values (Predictions)...\n",
      "tensor(-0.0316)\n",
      "tensor(-0.0323)\n"
     ]
    }
   ],
   "source": [
    "## Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = BasicLightningTrain() \n",
    "\n",
    "print(\"Before...\")\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values (Predictions)...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f688690c-bcce-4ce2-a6cb-630d3306e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1d6e510410484eac3160fa9e51417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "stage1shortW tensor(2.6675)\n",
      "stage1inputW tensor(1.5465)\n",
      "stage1B tensor(1.5411)\n",
      "stage2shortW1 tensor(1.8835)\n",
      "stage2inputW1 tensor(1.5822)\n",
      "stage2B1 tensor(0.5024)\n",
      "stage2shortW2 tensor(1.2124)\n",
      "stage2inputW2 tensor(0.8829)\n",
      "stage2B2 tensor(-0.3179)\n",
      "stage3shortW tensor(4.2505)\n",
      "stage3inputW tensor(-0.3005)\n",
      "stage3B tensor(0.4795)\n",
      "\n",
      "Output Values...\n",
      "tensor(-0.0511)\n",
      "tensor(0.9308)\n"
     ]
    }
   ],
   "source": [
    "## Since the initial predictions are bad, we will train the model\n",
    "## using a trainer.\n",
    "model = BasicLightningTrain() \n",
    "\n",
    "# trainer = L.Trainer(max_steps=600, log_every_n_steps=1)\n",
    "trainer = L.Trainer(max_epochs=4000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f15a5b-14a2-40ee-8478-741370d4252f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182800a-398f-4e92-8c02-a255fccb44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: We can run tensorboard inside this notebook or in it's own browser window.\n",
    "## When we run it in the notebook, it sometimes behaves funny. However, when we run it in its own browser window,\n",
    "## it works every time, so we'll give it's own browser window.\n",
    "##\n",
    "## To run tensorboard in it's own browser window...\n",
    "## Got to the \"File\" menu and select \"New Launcher\". Then scroll down and click on \"Terminal\"\n",
    "## In the terminal, navigate to the same directory that contains the \"lightning_logs\" directory.\n",
    "## Then in the terminal, enter...\n",
    "## \n",
    "## tensorboard --logdir=lightning_logs/\n",
    "##\n",
    "## ...this will then start the tensorboard server and will print out a URL (i.e. http://localhost:6007/ ). Copy the URL\n",
    "## and paste it into a new browser window and then you are good to go!!!\n",
    "##\n",
    "## NOTE: If you are feeling daring and want to run tensorboard inside this notebook just uncomment the code below:\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f36c6b-dead-45f2-9519-88109f75d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints/epoch=3999-step=8000.ckpt\n",
      "Restoring states from the checkpoint path at /Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints/epoch=3999-step=8000.ckpt\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:342: UserWarning: The dirpath has changed from '/Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints' to '/Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_9/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints/epoch=3999-step=8000.ckpt\n",
      "Restored all states from the checkpoint file at /Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints/epoch=3999-step=8000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /Users/joshstarmer/My Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_8/checkpoints/epoch=3999-step=8000.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff3318d366d4ce5a4524ffd1c8166ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 2it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "stage1shortW tensor(2.7043)\n",
      "stage1inputW tensor(1.6307)\n",
      "stage1B tensor(1.6234)\n",
      "stage2shortW1 tensor(1.9983)\n",
      "stage2inputW1 tensor(1.6525)\n",
      "stage2B1 tensor(0.6204)\n",
      "stage2shortW2 tensor(1.4122)\n",
      "stage2inputW2 tensor(0.9393)\n",
      "stage2B2 tensor(-0.3217)\n",
      "stage3shortW tensor(4.3848)\n",
      "stage3inputW tensor(-0.1943)\n",
      "stage3B tensor(0.5935)\n",
      "\n",
      "Output Values...\n",
      "tensor(-0.0781)\n",
      "tensor(0.9687)\n"
     ]
    }
   ],
   "source": [
    "## The logs suggest that maybe more training might help.\n",
    "## Maybe adding 1000 more epochs will improve the model a little bit more.\n",
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_best_checkpoint + \"\\n\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5000) # before the max epochs as 4000, so we're adding 1000 more\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3d9a6-e697-4293-a351-6983624d1458",
   "metadata": {},
   "source": [
    "The predictions, -0.08 for Company A and 0.97 for Company B, are pretty good considering the observed values were 0 and 1. However, we could probably improve the predictions even more if we ran the output of the LSTM through a fully connected neural network, where \"fully connected neural network\" is the most basic type of neural network and what comes to mind when most people say \"neural network\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9416be-e1e5-4706-943d-d60816f257a6",
   "metadata": {},
   "source": [
    "NOTE: These are the Weights and Biases used in the LSTM's Clearly Explained StatQuest...\n",
    "\n",
    "After...\n",
    "Parameters...\n",
    "stage1shortW tensor(2.7043)\n",
    "stage1inputW tensor(1.6307)\n",
    "stage1B tensor(1.6234)\n",
    "stage2shortW1 tensor(1.9983)\n",
    "stage2inputW1 tensor(1.6525)\n",
    "stage2B1 tensor(0.6204)\n",
    "stage2shortW2 tensor(1.4122)\n",
    "stage2inputW2 tensor(0.9393)\n",
    "stage2B2 tensor(-0.3217)\n",
    "stage3shortW tensor(4.3848)\n",
    "stage3inputW tensor(-0.1943)\n",
    "stage3B tensor(0.5935)\n",
    "\n",
    "Output Values...\n",
    "tensor(-0.0781)\n",
    "tensor(0.9687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc856973-3746-4d96-ad78-265a66538a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625e31f836b34ca99c2fd709902e10e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "stage1shortW tensor(2.5577)\n",
      "stage1inputW tensor(1.5359)\n",
      "stage1B tensor(1.5848)\n",
      "stage2shortW1 tensor(1.4833)\n",
      "stage2inputW1 tensor(1.9231)\n",
      "stage2B1 tensor(0.3202)\n",
      "stage2shortW2 tensor(1.8213)\n",
      "stage2inputW2 tensor(1.5784)\n",
      "stage2B2 tensor(-0.5503)\n",
      "stage3shortW tensor(4.5694)\n",
      "stage3inputW tensor(-0.1127)\n",
      "stage3B tensor(0.5870)\n",
      "\n",
      "Output Values...\n",
      "tensor(-0.1021)\n",
      "tensor(0.9868)\n"
     ]
    }
   ],
   "source": [
    "## Now let's demonstrate how much faster we can train the LSTM by setting the learning rate to 0.1\n",
    "## This will result in different Weights and Biases than I used in the \"LSMTs Clearly Explained\", but the\n",
    "## predictions are just as good, if not better.\n",
    "model = BasicLightningTrain() \n",
    "model.learning_rate = 0.1 # set the learning rate for Adam to 0.1\n",
    "\n",
    "# trainer = L.Trainer(max_steps=600, log_every_n_steps=1) \n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=1) # NOTE: By default L.Trainer() logs every 50 steps, and since we are only\n",
    "## doing 300 epochs, or 600 steps, that would mean logging only 12 times. So we tell L.Trainer() to log every step.\n",
    "\n",
    "## NOTE: Before we set max_expochs to 4000 and then did another 1000 epochs after that. Now we are only doing 600 steps, or \n",
    "## 300 epochs.\n",
    "# trainer = L.Trainer(max_epochs=4000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 1.])).detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b883e-ec87-475b-a760-57c651467cf0",
   "metadata": {},
   "source": [
    "BAM!!! By increasing the learning rate from 0.001 to 0.1, we went from needing 5000 epochs to get descent predictions to only needing 300 epochs. The predictions now are a little different, but just as good. We wanted 0 for Company A, and we got -0.1, and we wanted 1 for Company B and we got 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1051e4b9-8927-410f-a0f5-5c8f4b0c50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "        \n",
    "        self.hidden = (torch.zeros(1,1,1), # init hidden state (short-term memory) to 0\n",
    "                       torch.zeros(1,1,1)) # init cell state (long-term memory) to 0.\n",
    "\n",
    "        \n",
    "        self.state = 0 # this keeps track of which output we are trying to predict for logging\n",
    "    \n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector    \n",
    "        input_trans = input.view(len(input),1,-1)\n",
    "        \n",
    "        # print(\"input:\", str(input) + str(input.shape))\n",
    "        # print(\"input_trans:\", str(input_trans) + str(input_trans.shape))\n",
    "        \n",
    "        ## run it through the LSTM unit (which automatically unrolls for us)\n",
    "        # lstm_out, self.hidden = self.lstm(input_trans, self.hidden)\n",
    "        lstm_out, self.hidden = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ## logging...\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e350b3-cd46-4c58-ac62-6b205202e837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before...\n",
      "Parameters...\n",
      "lstm.weight_ih_l0 tensor([[ 0.1353],\n",
      "        [-0.8037],\n",
      "        [-0.3339],\n",
      "        [ 0.9626]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2466],\n",
      "        [-0.0502],\n",
      "        [-0.8303],\n",
      "        [-0.5594]])\n",
      "lstm.bias_ih_l0 tensor([-0.0204, -0.6212, -0.1240,  0.4070])\n",
      "lstm.bias_hh_l0 tensor([-0.9782,  0.2970, -0.6612, -0.4881])\n",
      "\n",
      "Output Values...\n",
      "tensor([[-0.1920]])\n",
      "tensor([[-0.1927]])\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "model_lstm = LightningLSTM() # First, make model from the class\n",
    "model_lstm(torch.tensor([0., 0.5, 0.25, 0.75]))\n",
    "print(\"Before...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model_lstm.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model_lstm(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model_lstm(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d14c6e-b345-430a-b6dd-fc34bab7dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3651087a6eb24253bdecc05216e05006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=600` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "lstm.weight_ih_l0 tensor([[3.5364],\n",
      "        [1.3869],\n",
      "        [1.5390],\n",
      "        [1.2488]])\n",
      "lstm.weight_hh_l0 tensor([[5.2070],\n",
      "        [2.9577],\n",
      "        [3.2652],\n",
      "        [2.0678]])\n",
      "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
      "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n",
      "\n",
      "Output Values...\n",
      "tensor([[-0.1887]])\n",
      "tensor([[0.9752]])\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed=42)\n",
    "\n",
    "model_lstm = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_steps=600, log_every_n_steps=1)\n",
    "trainer.fit(model_lstm, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model_lstm.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model_lstm(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(model_lstm(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c791951-e3a5-4631-8e6c-ab9234e82565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 63773), started 13:27:43 ago. (Use '!kill 63773' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ca154-97c4-4ff3-8990-793324f7532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4fb84e3-f2c5-497e-be56-ef5656c94247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters...\n",
      "weight_ih_l0 tensor([[-0.1648],\n",
      "        [-0.5693],\n",
      "        [-0.1617],\n",
      "        [ 0.8111]])\n",
      "weight_hh_l0 tensor([[-0.7420],\n",
      "        [ 0.2270],\n",
      "        [-0.9828],\n",
      "        [ 0.5243]])\n",
      "bias_ih_l0 tensor([0.3695, 0.0424, 0.4292, 0.0011])\n",
      "bias_hh_l0 tensor([ 0.5534, -0.7916, -0.1469,  0.4436])\n"
     ]
    }
   ],
   "source": [
    "test = nn.LSTM(input_size=1, hidden_size=1)\n",
    "print(\"Parameters...\")\n",
    "for name, param in test.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02784743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
