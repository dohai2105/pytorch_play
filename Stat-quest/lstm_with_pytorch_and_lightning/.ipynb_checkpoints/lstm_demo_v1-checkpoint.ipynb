{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613cef3c-fd09-4f12-8248-72a2e1bc1d23",
   "metadata": {},
   "source": [
    "This is, in theory, a super simple example of how Long Short-Term Memory Neural Networks work. We'll start by implementing a single \"memory cell\" that we'll duplicate (reusing all the weights and biases) for each element in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "First, import the modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "# from torch.optim import SGD # optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.\n",
    "from torch.optim import Adam # optim contains many optimizers. Here, we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5d37b-7be6-46d1-8978-e9f956ae3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.normal(mean=torch.tensor(0.0), std=torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLightningTrain(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        # self.train_acc = Accuracy()\n",
    "        \n",
    "#         self.wf1 = nn.Parameter(torch.tensor(-3.2), requires_grad=True)\n",
    "#         self.wf2 = nn.Parameter(torch.tensor(1.7), requires_grad=True)\n",
    "#         self.bf1 = nn.Parameter(torch.tensor(-0.85), requires_grad=True)\n",
    "\n",
    "#         self.wr1 = nn.Parameter(torch.tensor(1.3), requires_grad=True)\n",
    "#         self.wr2 = nn.Parameter(torch.tensor(2.1), requires_grad=True)\n",
    "#         self.br1 = nn.Parameter(torch.tensor(1.5), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.tensor(-0.2), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.tensor(-0.3), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.tensor(-0.05), requires_grad=True)\n",
    "        \n",
    "#         self.wo1 = nn.Parameter(torch.tensor(0.4), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.tensor(1.24), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.tensor(0.31), requires_grad=True)\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        self.wf1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wf2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bf1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.br1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        # self.learning_rate = 0.01 \n",
    "        self.state = 0\n",
    "        \n",
    "    def forward(self, input): \n",
    "        \n",
    "        cell_state = 0\n",
    "        hidden_state = 0\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        # print(\"day1 \" + str(day1))\n",
    "        # print(\"day2 \" + str(day2))\n",
    "        # print(\"day3 \" + str(day3))\n",
    "        # print(\"day4 \" + str(day4))\n",
    "        \n",
    "        ##############\n",
    "        ##\n",
    "        ## Day 1\n",
    "        ##\n",
    "        ##############\n",
    "        forget_percent = F.sigmoid((hidden_state * self.wf1) + (day1 * self.wf2) + self.bf1)\n",
    "        remember_percent = F.sigmoid((hidden_state * self.wr1) + (day1 * self.wr2) + self.br1)\n",
    "        potential_memory = F.tanh((hidden_state * self.wp1) + (day1 * self.wp2) + self.bp1)\n",
    "        output_percent = F.sigmoid((hidden_state * self.wo1) + (day1 * self.wo2) + self.bo1)\n",
    "        \n",
    "        cell_state = (cell_state * forget_percent) + (remember_percent * potential_memory)\n",
    "        hidden_state = F.tanh(cell_state) * output_percent\n",
    "        \n",
    "        \n",
    "        ##############\n",
    "        ##\n",
    "        ## Day 2\n",
    "        ##\n",
    "        ##############\n",
    "        forget_percent = F.sigmoid((hidden_state * self.wf1) + (day2 * self.wf2) + self.bf1)\n",
    "        remember_percent = F.sigmoid((hidden_state * self.wr1) + (day2 * self.wr2) + self.br1)\n",
    "        potential_memory = F.tanh((hidden_state * self.wp1) + (day2 * self.wp2) + self.bp1)\n",
    "        output_percent = F.sigmoid((hidden_state * self.wo1) + (day2 * self.wo2) + self.bo1)\n",
    "        \n",
    "        cell_state = (cell_state * forget_percent) + (remember_percent * potential_memory)\n",
    "        hidden_state = F.tanh(cell_state) * output_percent\n",
    "        \n",
    "        ##############\n",
    "        ##\n",
    "        ## Day 3\n",
    "        ##\n",
    "        ##############\n",
    "        forget_percent = F.sigmoid((hidden_state * self.wf1) + (day3 * self.wf2) + self.bf1)\n",
    "        remember_percent = F.sigmoid((hidden_state * self.wr1) + (day3 * self.wr2) + self.br1)\n",
    "        potential_memory = F.tanh((hidden_state * self.wp1) + (day3 * self.wp2) + self.bp1)\n",
    "        output_percent = F.sigmoid((hidden_state * self.wo1) + (day3 * self.wo2) + self.bo1)\n",
    "        \n",
    "        cell_state = (cell_state * forget_percent) + (remember_percent * potential_memory)\n",
    "        hidden_state = F.tanh(cell_state) * output_percent\n",
    "        \n",
    "        ##############\n",
    "        ##\n",
    "        ## Day 4\n",
    "        ##\n",
    "        ##############\n",
    "        forget_percent = F.sigmoid((hidden_state * self.wf1) + (day4 * self.wf2) + self.bf1)\n",
    "        remember_percent = F.sigmoid((hidden_state * self.wr1) + (day4 * self.wr2) + self.br1)\n",
    "        potential_memory = F.tanh((hidden_state * self.wp1) + (day4 * self.wp2) + self.bp1)\n",
    "        output_percent = F.sigmoid((hidden_state * self.wo1) + (day4 * self.wo2) + self.bo1)\n",
    "        \n",
    "        cell_state = (cell_state * forget_percent) + (remember_percent * potential_memory)\n",
    "        hidden_state = F.tanh(cell_state) * output_percent\n",
    "    \n",
    "        ##### Now return the hidden_state as the output\n",
    "        output = hidden_state\n",
    "        return output\n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return SGD(self.parameters(), lr=self.learning_rate)\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        \n",
    "        ## NOTE: When training_step() is called it calculates the loss with the code below...\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        # self.train_acc.update(output_i, label_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        ##...before calling (internally and behind the scenes)...\n",
    "        ## optimizer.zero_grad() # to clear gradients\n",
    "        ## loss.backward() # to do the backpropagation\n",
    "        ## optimizer.step() # to update the parameters\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "# labels = torch.tensor([0, 1])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLightningTrain() # First, make model from the class\n",
    "print(\"Before...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688690c-bcce-4ce2-a6cb-630d3306e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=4000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f36c6b-dead-45f2-9519-88109f75d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we are close. How do I add an additional 1000 epochs without starting over?\n",
    "## 3 Features:\n",
    "## 1) Picking up where we left off\n",
    "## 2) Logging and visualizing the logs (perhaps with comet or tensorboard).\n",
    "## 3) LSTM module?\n",
    "## - ask the team?\n",
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_best_checkpoint + \"\\n\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051e4b9-8927-410f-a0f5-5c8f4b0c50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e705219c-78f9-4237-8e57-6e3a5e9a5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just learning how the dataloader works...\n",
    "\n",
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "# labels = torch.tensor([0, 1])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "# dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977a9e58-24b8-45fe-8914-01eb51ab3594",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09cc62bd-20a0-47b7-9302-bf72b84f7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "batch: [tensor([[0.0000, 0.5000, 0.2500, 1.0000],\n",
      "        [1.0000, 0.5000, 0.2500, 1.0000]]), tensor([0., 1.])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch))\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mbatch shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(\"index: \" + str(i))\n",
    "    print(\"batch: \" + str(batch))\n",
    "    print(\"\\tbatch shape: \" + str(batch.shape))\n",
    "    print(\"input: \" + str(batch[0]))\n",
    "    print()\n",
    "    for j, value in enumerate(batch[0][0]):\n",
    "        print(\"\\tday: \" + str(j) + \" \" + str(value))\n",
    "    print(\"\\nlabel: \" + str(batch[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671fa1fb-afc2-48bf-97d7-240baf5eed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5000, 0.2500, 1.0000]])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "batch, y = next(iter(dataloader))\n",
    "print(batch)\n",
    "print(batch.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6dcf43-ebf8-4325-83c4-9eceef8376f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: tensor([[0.0000, 0.5000, 0.2500, 1.0000]])\n",
      "batch.size(): torch.Size([1, 4]) \n",
      "\n",
      "y: tensor([0.])\n",
      "y.size(): torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "batch, y = next(iter(dataloader))\n",
    "print(\"batch:\", str(batch))\n",
    "print(\"batch.size():\", str(batch.size()), \"\\n\")\n",
    "print(\"y:\", str(y))\n",
    "print(\"y.size():\", str(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a63b5c-d4f2-4ae8-a433-4889e92cad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLightningTrain() # First, make model from the class\n",
    "\n",
    "## Now create a Trainer - we can use the trainer to...\n",
    "##  1) Find the optimal learning rate\n",
    "##  2) Train (optimize) the weights and biases in the model\n",
    "## By default, the trainer will run on your system's CPU\n",
    "trainer = L.Trainer(max_epochs=34) \n",
    "## However, if we wanted to automatically take advantage of any available GPUs,\n",
    "## we would set accelerator=\"auto\" to automatically use available GPUs\n",
    "## and we would set devices=\"auto\" to automatically select as many GPUs as we have.\n",
    "#\n",
    "# trainer = L.Trainer(max_epochs=34, accelerator=\"auto\", devices=\"auto\")\n",
    "\n",
    "## Now let's find the optimal learning rate\n",
    "lr_find_results = trainer.tuner.lr_find(model,\n",
    "                                        train_dataloaders=dataloader, # the training data\n",
    "                                        min_lr=0.001, # minimum learning rate\n",
    "                                        max_lr=1.0,   # maximum learning rate\n",
    "                                        early_stop_threshold=None) # setting this to \"None\" tests all 100 candidate rates\n",
    "new_lr = lr_find_results.suggestion() ## suggestion() returns the best guess for the optimal learning rate\n",
    "\n",
    "## now print out the learning rate\n",
    "print(f\"lr_find() suggests {new_lr:.5f} for the learning rate.\")\n",
    "\n",
    "# now set the model's learning rate to the new value\n",
    "model.learning_rate = new_lr\n",
    "\n",
    "## NOTE: we can also plot the loss for each learning rate tested.\n",
    "## When you have a lot of data, this graph can be useful\n",
    "## (see https://pytorch-lightning.readthedocs.io/en/1.4.5/advanced/lr_finder.html to learn how to interpret)\n",
    "## but when you only have 3 data points, like our example, this plot is pretty hard to interpret so I did\n",
    "## not cover it in the video.\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c5d1e-572e-4b41-9934-96cdaa7bae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have an improved learning rate, we can train the model (optimize final_bias)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(model.final_bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c00c18-47b2-4c4e-924c-8c506caba175",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the different doses through the neural network\n",
    "output_values = model(input_doses)\n",
    "\n",
    "## set the style for seaborn so that the graph looks cool.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "## create the graph (you might not see it at this point, but you will after we save it as a PDF).\n",
    "sns.lineplot(x=input_doses, \n",
    "             y=output_values.detach(), ## NOTE: we call detach() because final_bias has a gradient\n",
    "             color='green', \n",
    "             linewidth=2.5)\n",
    "\n",
    "## now label the y- and x-axes.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "\n",
    "## lastly, save the graph as a PDF.\n",
    "# plt.savefig('BasicLightningTrain_optimized.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
