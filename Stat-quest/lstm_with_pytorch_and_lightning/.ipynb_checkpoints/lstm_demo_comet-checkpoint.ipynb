{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613cef3c-fd09-4f12-8248-72a2e1bc1d23",
   "metadata": {},
   "source": [
    "This is, in theory, a super simple example of how Long Short-Term Memory Neural Networks work. We'll start by implementing a single \"memory cell\" that we'll duplicate (reusing all the weights and biases) for each element in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "First, import the modules..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "# from torch.optim import SGD # optim contains many optimizers. Here, we're using SGD, stochastic gradient descent.\n",
    "from torch.optim import Adam # optim contains many optimizers. Here, we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw graphs.\n",
    "import seaborn as sns ## seaborn makes it easier to draw nice-looking graphs.\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLightningTrain(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        self.wf1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wf2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bf1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.br1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## PyTorch's nn.LSTM() uses a uniform distribution to initialize weights and biases\n",
    "        ## so we can simulate that here...\n",
    "#         self.wf1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wf2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bf1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.br1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "        self.state = 0 # this is used to keep track of which output we are trying to predict\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        ## NOTES:\n",
    "        ## long term memory is also called \"cell state\"\n",
    "        ## short term memory is also called \"hidden state\"\n",
    "        forget_percent = torch.sigmoid((short_memory * self.wf1) + (input_value * self.wf2) + self.bf1)\n",
    "        remember_percent = torch.sigmoid((short_memory * self.wr1) + (input_value * self.wr2) + self.br1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + (input_value * self.wp2) + self.bp1)\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + (input_value * self.wo2) + self.bo1)\n",
    "        \n",
    "        long_memory = (long_memory * forget_percent) + (remember_percent * potential_memory)\n",
    "        short_memory = torch.tanh(long_memory) * output_percent\n",
    "        return([long_memory, short_memory])\n",
    "        \n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        long_memory = 0 # long term memory is also called \"cell state\" also called c0\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" also called h0\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        \n",
    "        ## Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        ##### Now return short_memory\n",
    "        return short_memory # final value for h4\n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        \n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        # self.train_acc.update(output_i, label_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        ## Internally and behind the scenes, Lightning now calls...\n",
    "        ## optimizer.zero_grad() # to clear gradients\n",
    "        ## loss.backward() # to do the backpropagation\n",
    "        ## optimizer.step() # to update the parameters\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before...\n",
      "Parameters...\n",
      "wf1 tensor(0.3367)\n",
      "wf2 tensor(0.1288)\n",
      "bf1 tensor(0.)\n",
      "wr1 tensor(0.2345)\n",
      "wr2 tensor(0.2303)\n",
      "br1 tensor(0.)\n",
      "wp1 tensor(-1.1229)\n",
      "wp2 tensor(-0.1863)\n",
      "bp1 tensor(0.)\n",
      "wo1 tensor(2.2082)\n",
      "wo2 tensor(-0.6380)\n",
      "bo1 tensor(0.)\n",
      "\n",
      "Output Values...\n",
      "tensor(-0.0316)\n",
      "tensor(-0.0323)\n"
     ]
    }
   ],
   "source": [
    "model = BasicLightningTrain() # First, make model from the class\n",
    "print(\"Before...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f688690c-bcce-4ce2-a6cb-630d3306e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e9a44994bd45e8907215d63c0d2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "wf1 tensor(2.6675)\n",
      "wf2 tensor(1.5465)\n",
      "bf1 tensor(1.5411)\n",
      "wr1 tensor(1.8835)\n",
      "wr2 tensor(1.5822)\n",
      "br1 tensor(0.5024)\n",
      "wp1 tensor(1.2124)\n",
      "wp2 tensor(0.8829)\n",
      "bp1 tensor(-0.3179)\n",
      "wo1 tensor(4.2505)\n",
      "wo2 tensor(-0.3005)\n",
      "bo1 tensor(0.4795)\n",
      "\n",
      "Output Values...\n",
      "tensor(-0.0511)\n",
      "tensor(0.9308)\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=4000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69aa348-8780-427b-ba74-a7807ab18f9a",
   "metadata": {},
   "source": [
    "Output Values...\n",
    "tensor([-0.1503])\n",
    "tensor([0.9781])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f36c6b-dead-45f2-9519-88109f75d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we are close. But maybe adding 1000 more epochs will improve the model a little bit more.\n",
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_best_checkpoint + \"\\n\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5000) # before the max epochs as 4000, so we're adding 1000 more\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91635ca4-436c-4062-8e42-71fcd8f39da5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output Values...\n",
    "tensor([-0.1613])\n",
    "tensor([0.9859])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051e4b9-8927-410f-a0f5-5c8f4b0c50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's see what we can do with nn.LSTM()\n",
    "\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = number of LSTMs we want to connect the input to. This is \n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "        \n",
    "        self.hidden = (torch.zeros(1,1,1), # init hidden state (short-term memory) to 0\n",
    "                       torch.zeros(1,1,1)) # init cell state (long-term memory) to 0.\n",
    "\n",
    "        \n",
    "        self.state = 0 # this keeps track of which output we are trying to predict for logging\n",
    "    \n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector    \n",
    "        input_trans = input.view(len(input),1,-1)\n",
    "        \n",
    "        print(\"input:\", str(input) + str(input.shape))\n",
    "        print(\"input_trans:\", str(input_trans) + str(input_trans.shape))\n",
    "        \n",
    "        ## run it through the LSTM unit (which automatically unrolls for us)\n",
    "        # lstm_out, self.hidden = self.lstm(input_trans, self.hidden)\n",
    "        lstm_out, self.hidden = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        \n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        # self.train_acc.update(output_i, label_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        ## Internally and behind the scenes, Lightning now calls...\n",
    "        ## optimizer.zero_grad() # to clear gradients\n",
    "        ## loss.backward() # to do the backpropagation\n",
    "        ## optimizer.step() # to update the parameters\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f2f15b5-ef31-48aa-81c2-a6bd13eee644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([0.0000, 0.5000, 0.2500, 0.7500])torch.Size([4])\n",
      "input_trans: tensor([[[0.0000]],\n",
      "\n",
      "        [[0.5000]],\n",
      "\n",
      "        [[0.2500]],\n",
      "\n",
      "        [[0.7500]]])torch.Size([4, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6228]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm = LightningLSTM() # First, make model from the class\n",
    "model_lstm(torch.tensor([0., 0.5, 0.25, 0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6e350b3-cd46-4c58-ac62-6b205202e837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before...\n",
      "Parameters...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Output Values...\n",
      "tensor([[0.6228]])\n",
      "tensor([[0.6219]])\n"
     ]
    }
   ],
   "source": [
    "model_lstm = LightningLSTM() # First, make model from the class\n",
    "model_lstm(torch.tensor([0., 0.5, 0.25, 0.75]))\n",
    "print(\"Before...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model_lstm.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model_lstm(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model_lstm(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27d14c6e-b345-430a-b6dd-fc34bab7dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1de45af3b8249b4b93c39f7e0a20d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After...\n",
      "Parameters...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7622],\n",
      "        [ 0.6810],\n",
      "        [-0.2552],\n",
      "        [ 0.7721]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2550],\n",
      "        [ 0.0546],\n",
      "        [-0.5445],\n",
      "        [ 0.4085]])\n",
      "lstm.bias_ih_l0 tensor([ 0.7066, -0.9065,  0.7998,  0.0915])\n",
      "lstm.bias_hh_l0 tensor([ 0.5639, -0.0374,  0.4129, -0.2369])\n",
      "\n",
      "Output Values...\n",
      "tensor([[0.4641]])\n",
      "tensor([[0.4632]])\n"
     ]
    }
   ],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_epochs=4000, log_every_n_steps=1)\n",
    "trainer.fit(model_lstm, train_dataloaders=dataloader)\n",
    "print(\"\\nAfter...\")\n",
    "## print out the name and value for each parameter\n",
    "print(\"Parameters...\")\n",
    "for name, param in model_lstm.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nOutput Values...\")\n",
    "print(model_lstm(torch.tensor([0., 0.5, 0.25, 0.75])).detach())\n",
    "print(model_lstm(torch.tensor([1., 0.5, 0.25, 0.75])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c791951-e3a5-4631-8e6c-ab9234e82565",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4fb84e3-f2c5-497e-be56-ef5656c94247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters...\n",
      "weight_ih_l0 tensor([[-0.5290],\n",
      "        [ 0.2060],\n",
      "        [-0.1323],\n",
      "        [-0.6332],\n",
      "        [ 0.3229],\n",
      "        [-0.1645],\n",
      "        [-0.2942],\n",
      "        [-0.2197]])\n",
      "weight_hh_l0 tensor([[ 0.4808,  0.4155],\n",
      "        [ 0.1926,  0.2959],\n",
      "        [ 0.3632,  0.2925],\n",
      "        [ 0.6021,  0.4258],\n",
      "        [-0.3639, -0.1077],\n",
      "        [ 0.3298,  0.5626],\n",
      "        [-0.5876, -0.3507],\n",
      "        [-0.3507,  0.6679]])\n",
      "bias_ih_l0 tensor([ 0.6940, -0.3161, -0.2185,  0.0125,  0.0414, -0.2015, -0.5159, -0.3763])\n",
      "bias_hh_l0 tensor([-0.0590,  0.0765,  0.0271, -0.6493,  0.6261, -0.4119, -0.1731,  0.6559])\n"
     ]
    }
   ],
   "source": [
    "test = nn.LSTM(input_size=1, hidden_size=2)\n",
    "print(\"Parameters...\")\n",
    "for name, param in test.named_parameters():\n",
    "    print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
