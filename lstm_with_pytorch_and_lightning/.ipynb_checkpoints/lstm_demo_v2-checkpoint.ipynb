{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Short-Term Memory with PyTorch + Lightning!!!\n",
    "## Sponsored by...\n",
    "[<img src=\"./images/Brandmark_FullColor_Black.png\" alt=\"Lightning\" style=\"width: 400px;\">](https://www.pytorchlightning.ai/)\n",
    "\n",
    "Copyright 2022, Joshua Starmer\n",
    "\n",
    "# 3 MAIN IDEAS FOR THE LSTM STATQUEST\n",
    "- How to use TensorBoard to see how the model traied and decide if you should try adding more epochs to training\n",
    "- How to add extra expochs without having to start over\n",
    "- How to use PyTorch LSTM class torch.nn.LSTM()\n",
    "\n",
    "# Questions\n",
    "- Why does PyTorch nn.LSTM() have extra bias terms in the equations? They have two bias terms per equation instead of the standard 1.\n",
    "- Is there an easy way to clean up the \"lightning_logs\" (delete them etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "---- \n",
    "**NOTE:** This tutorial is from the StatQuest **[Long Short-Term Memory with PyTorch + Lightning]()**.\n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) + [Lightning](https://www.lightning.ai/)** to create, optimize and make predictions using the Long Short-Term Memory network featured in the StatQuest **[Long Short-Term Memory, Clearly Explained!!!](https://youtu.be/YCzL96nL7j0)** In that StatQuest, we implemented the Long Short-Term Memory Unit, seen below, that uses predicts sequential data to predict the value of two different companies.\n",
    "<!-- <img src=\"./xgboost_tree.png\" alt=\"An XGBoost Tree\" style=\"width: 600px;\"> -->\n",
    "\n",
    "<img src=\"./images/lstm_image.001.png\" alt=\"A Long Short-Term Memory Unit\" style=\"width: 1620px;\">\n",
    "\n",
    "The training data (below) consist of stock prices for two different companies, Company A and Company B. The goal is to use the data from the first 4 days to predict what the price will be on the 5th day. If you look closely at the data, you'll see that the only differences in the prices occur on Day 1 and Day 5. So the LSTM has to remember what happened on Day 1 in order to predict what will happen on Day 5.\n",
    "\n",
    "\n",
    "<img src=\"./images/company_a_data.png\" alt=\"Data for Company A\" style=\"width: 450px;\"> <img src=\"./images/company_b_data.png\" alt=\"Data for Company B\" style=\"width: 450px;\">\n",
    "\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Build a Long Short-Term Memory (LSTM) unit by hand with Lightning](#build)**\n",
    "\n",
    "- **[Train the LSTM unit and use Lightning and TensorBoard to evaluate and add additional epochs to the training without starting over](#train)**\n",
    "\n",
    "- **[Build a Long Short-Term Memory Unit with nn.LSTM() and train it with Lightning](#using)**\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and are familiar with the <!-- basics of **[PyTorch](https://youtu.be/FHdlXe1bSe4)** and the  --> theory behind **[Neural Networks](https://youtu.be/CqOfi41LfDw)**, **[Backpropagation](https://youtu.be/IN2XmBhILt4)**, and **[Long Short-Term Memory](https://youtu.be/YCzL96nL7j0)**. If not, check out the **'Quests** by clicking on the links for each topic.\n",
    "\n",
    "#### ALSO NOTE:\n",
    "I strongly encourage you to play around with the code. Playing with the code is the best way to learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "**TL;DR** For the most part, this is the same as the **The StatQuest Introduction to Coding Neural Networks with PyTorch + Lightning**, except now we don't have to draw our own graphs, so we are omitting `matplotlib` and `seaborn`. Also, instead of using **Stochastic Gradient Descent** to optimize the weights and biaes, we are using **Adam** and we're also importing a function that allows us to set the seed for the random numbers we use to initialize the weights and biases.\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create a neural network, use and graph the output for various input values, and optimize the neural network's parameters.\n",
    "\n",
    "**NOTE:** You will need **Python 3.8** and have at least these versions for each of the following modules: \n",
    "- pytorch >= 1.10.1\n",
    "- lightning >= 1.8.0\n",
    "\n",
    "### If you installed **Python** with [Anaconda](https://www.anaconda.com/)...\n",
    "...then you can check which versions of each package you have with the command: `conda list`. If, for example, your version of `matplotlib` is older than **3.3.4**, then the easiest thing to do is just update all of your Anaconda packages with the following command: `conda update --all`. However, if you only want to update `matplotlib`, then you can run this command: `conda install matplotlib=3.3.4`.\n",
    "\n",
    "### If you need to install **PyTorch**...\n",
    "...then the easiest thing to do is follow the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "### If you need to install **Lightning**...\n",
    "...then the easiest thing to do is follow the instructions on the [Lightning AI website](https://lightning.ai/lightning-docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1076357-c957-44ed-947b-8267b4f4c3a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# Build a Long Short-Term Memory unit by hand using PyTorch + Lightning\n",
    "\n",
    "Just like we have done in previous tutorials, building a neural network (and a Long Short-Term Memory unit is a type of neural network) we need to create a new class. To make it easy to train the LSTM, we'll make this class inherit from `LightningModule` and we'll create the following methods:\n",
    "- `__init__()` to initialize the weights and biases and keep track of a few other house keeping things.\n",
    "- `lstm_unit()` to do the LSTM math.\n",
    "- `forward()` to make a forward pass through the unrolled LSTM. In other words `forward()` calls `lstm_unit()` for each data point.\n",
    "- `configure_optimizers()` to configure the opimimizer. In the past, we have use `SGD` (Stochastic Gradient Descent), however, in this tutorial we'll change things up and use `Adam`, another popular algorithm for optimizing the weights and biases.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and to keep track of the loss values in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are implementing an LSTM network by hand...\n",
    "class LSTMbyHand(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## The first thing we do is set the seed for the random number generorator.\n",
    "        ## This ensures that when someone crease and model from this class, that model\n",
    "        ## will start off with the exact same random numbers as I started out when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the tensors for the LSTM\n",
    "        ##\n",
    "        ###################\n",
    "        \n",
    "        ## NOTE: nn.LSTM() uses random values from a uniform distribution to initialize the tensors\n",
    "        ## Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
    "        ## We'll start with the Normal Distribtion...\n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)        \n",
    "        \n",
    "        ## NOTE: In this case, I'm only using the normal distribution for the Weights.\n",
    "        ## All Biases are initialized to 0.\n",
    "        self.wf1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wf2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bf1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.br1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        ## We can also initialize all Weights and Biases using a uniform distribution. This is\n",
    "        ## how nn.LSTM() does it.\n",
    "#         self.wf1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wf2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bf1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.br1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "\n",
    "#         self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "#         self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "#         self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Initialize the state for the logger\n",
    "        ##\n",
    "        ###################\n",
    "        \n",
    "        ## Because our LSTM has two sequences of training data, one from Company A that has 0 as the\n",
    "        ## Day 5 value, and one from Company B that has 1 as the Day 5 value, we're going to use\n",
    "        ## a variable, self.state, to keep track of which company the LSTM is trying to predict.\n",
    "        ## This will allow us to log the predictions for Company A in a separate file from the\n",
    "        ## predictions for Company B, and, as a result, this will make it easier to evaluate how\n",
    "        ## well the training went for each company.\n",
    "        self.state = 0\n",
    "        \n",
    "        \n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        ## lstm_unit does the math for a single LSTM unit.\n",
    "        \n",
    "        ## NOTES:\n",
    "        ## long term memory is also called \"cell state\"\n",
    "        ## short term memory is also called \"hidden state\"\n",
    "        \n",
    "        ## 1) The first stage determines what percent of the current long-term memory\n",
    "        ##    should be remembered\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wf1) + \n",
    "                                              (input_value * self.wf2) + \n",
    "                                              self.bf1)\n",
    "        \n",
    "        ## 2) The second stage creates a new, potential long-term memory and determines what\n",
    "        ##    percentage of that to add to the current long-term memory\n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wr1) + \n",
    "                                                   (input_value * self.wr2) + \n",
    "                                                   self.br1)\n",
    "        potential_memory = torch.tanh((short_memory * self.wp1) + \n",
    "                                      (input_value * self.wp2) + \n",
    "                                      self.bp1)\n",
    "        \n",
    "        ## Once we have gone through the first two stages, we can update the long-term memory\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + \n",
    "                       (potential_remember_percent * potential_memory))\n",
    "        \n",
    "        ## 3) The third stage create a new, potential short-term memory determines what\n",
    "        ##    percentage of that should be remembered and used as output.\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1) + \n",
    "                                       (input_value * self.wo2) + \n",
    "                                       self.bo1)         \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "        \n",
    "        return([updated_long_memory, updated_short_memory])\n",
    "        \n",
    "    \n",
    "    def forward(self, input): \n",
    "        ## forward() unrolls the LSTM for the training data by calling lstm_unit() for each day of training data \n",
    "        ## that we have. forward() also keeps track of the long and short-term memories after each day and returns\n",
    "        ## the final short-term memory, which is the 'output' of the LSTM.\n",
    "        \n",
    "        long_memory = 0 # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
    "        short_memory = 0 # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "        \n",
    "        ## Day 1\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 2\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 3\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        \n",
    "        ## Day 4\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "        \n",
    "        ##### Now return short_memory, which is the 'output' of the LSTM.\n",
    "        return short_memory # final value for h4\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        # return Adam(self.parameters(), lr=0.1) # setting the learning rate to 0.1 trains way faster than\n",
    "                                                 # using the default learning rate, lr=0.001, which requires a lot more \n",
    "                                                 # training. However, if we use the default value, we get \n",
    "                                                 # the exact same Weights and Biases that I used in\n",
    "                                                 # the LSTM Clearly Explained StatQuest video. So we'll use the\n",
    "                                                 # default value.\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ## logging...\n",
    "        self.log(\"train_loss\", loss)\n",
    "        ## NOTE: Our dataset consists of two sequences of values representing Company A and Company B\n",
    "        ## For Company A, the goal is to predict that the value on Day 5 = 0, and for Company B,\n",
    "        ## the goal is to predict that the value on Day 5 = 1. We use \"self.state\" to keep track of\n",
    "        ## which company we just made a prediction for and log that output value so we can see how\n",
    "        ## well we are predicting each company's value.\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb",
   "metadata": {},
   "source": [
    "Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized weights and biases. Then, just for fun, we'll see what those random weights and biases predict for Company A and Company B. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3ea653-f746-46db-9029-44242a1b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "wf1 tensor(0.3367)\n",
      "wf2 tensor(0.1288)\n",
      "bf1 tensor(0.)\n",
      "wr1 tensor(0.2345)\n",
      "wr2 tensor(0.2303)\n",
      "br1 tensor(0.)\n",
      "wp1 tensor(-1.1229)\n",
      "wp2 tensor(-0.1863)\n",
      "bp1 tensor(0.)\n",
      "wo1 tensor(2.2082)\n",
      "wo2 tensor(-0.6380)\n",
      "bo1 tensor(0.)\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(-0.0377)\n",
      "Company B: Observed = 1, Predicted = tensor(-0.0383)\n"
     ]
    }
   ],
   "source": [
    "## Create the model object, print out parameters and see how well\n",
    "## the untrained LSTM can make predictions...\n",
    "model = LSTMbyHand() \n",
    "\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329d6f2-39ff-4f9b-bc99-fbd1a2013815",
   "metadata": {},
   "source": [
    "Now, the predicted value for Company A, -0.0377, isn't terrible, since it is relatively close to the observed value, 0. However, the predicted value for Company B, -0.0383, _is_ terrible, because it is relatively far from the observed value, 1. So, that means we need to train the LSTM.\n",
    "\n",
    "Small bam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1116de-c45d-4d82-a727-73a5806fa18f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0529b9c-c550-4fc6-a424-069e34b29c4b",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Optimizing (Training) the weights and biases in the LSTM that we made by hand: Part 1 - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e2424-649a-44ce-8a34-c98b8fced609",
   "metadata": {},
   "source": [
    "Since we are using **Lightning** training our homemade LSTM is pretty easy. All we have to do is create the training data and put it into a `DataLoader`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea2ef9-7fe2-4bfe-8645-4db921a4b20c",
   "metadata": {},
   "source": [
    "...and then create a Lightning Trainer, `L.Trainer`, and fit it to the training data. **NOTE:** We starting with 2000 epochs. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out when we compare the predictions to the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10157397-3100-4d18-be96-cdc294ce89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1555: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0532e95e3923453aaff035076bcdd902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=2000) # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586093b-38e5-431a-a7f0-ad65f2a4c979",
   "metadata": {},
   "source": [
    "Now that we've trained the model with 2000 epochs, we can see how good the predictions are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d40609-2e07-4152-ae6c-7d32cc99e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.4342)\n",
      "Company B: Observed = 1, Predicted = tensor(0.6171)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292d775-336f-41b7-838c-b297c49693a8",
   "metadata": {},
   "source": [
    "Unfortunately, these predictions are terrible. :( So we'll have to do more training. However, before we dive into more training, let's look at the loss values and predictions that we saved in log files with **TensorBoard**. **TensorBoard** will graph everything that we logged during training, making it super easy to see if things are headed in the right direction or not.\n",
    "\n",
    "To get TensorBoard working\n",
    "- Go to the \"File\" menu and select \"New Launcher\"\n",
    "- Scroll down and click on \"Terminal\"\n",
    "- In the terminal, navigate to the same directory that contains the \"lightning_logs\" directory.\n",
    "- Then in the terminal, enter `tensorboard --logdir=lightning_logs/`\n",
    "\n",
    "this will then start the tensorboard server and will print out a URL (i.e. http://localhost:6007/ ). Copy the URL\n",
    "and paste it into a new browser window and then you are good to go!!! BAM!!!\n",
    "\n",
    "Below are the graphs of **loss** (train_loss), the predictions for Company A (out_0), and the predictions for Company B (out_1). Remember for Companay A, we want to predict 0 and for Company B, we want to predict 1.\n",
    "\n",
    "<img src=\"./images/train_loss_2000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_2000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_2000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "If we look at the loss (train_loss), we see that it is going down, which is good, but it still has further to go. When we look at the predictions for Company A (out_0), we see that they started out pretty good, close to 0, but then got really bad early on in training, shooting all the way up to 0.5, but are starting to get smaller. In contrast, when we look at the predictions for Company A (out_1), we see that they started out really bad, close to 0, but have been getting better ever since and look like they could continue to get better if we kept training.\n",
    "\n",
    "In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve. So let's add more epochs to the training. **NOTE:** Because we're using **Lightning**, we can pick up where we left off in training without having to start over from scratch. This is awesome and will save us a lot of time. So let's add an additional 1000 epochs to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed041-4f46-4dc6-bd6f-7d2bf01886a4",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Optimizing (Training) the weights and biases in the LSTM that we made by hand: Part 2 - Adding More Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ff55c1-cdc2-4086-b0d4-8d8333058674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:346: UserWarning: The dirpath has changed from '/Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_0/checkpoints' to '/Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1555: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e2b7e75b54b35b7770aa3bce4fb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 2it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
     ]
    }
   ],
   "source": [
    "## The logs suggest that maybe more training might help.\n",
    "## Maybe adding 1000 more epochs will improve the model a little bit more.\n",
    "\n",
    "## In order to pick up from where we left off, we have to tell the trainer where the most recent checkpoint files are saved\n",
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_best_checkpoint + \"\\n\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=3000) # Before, max_epochs=2000, so, by setting it to 3000, we're adding 1000 more.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f6af6-7d7e-4c40-9bbf-f467114d838e",
   "metadata": {},
   "source": [
    "Now that we have added 1000 epochs to the training, let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c805e2b-27ae-4302-af26-5fe0d55f6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.2708)\n",
      "Company B: Observed = 1, Predicted = tensor(0.7534)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42455342-f774-43a7-a043-536099d39b01",
   "metadata": {},
   "source": [
    "...and they are much better than before. Hooray!!! We can also check the logs with TensorBoard to see if it makes sense to add more epochs to the training. Since we already have TensorBoard running in a separate browser window, all we have to do is reload that page to update the graphs (below).\n",
    "\n",
    "<img src=\"./images/train_loss_3000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_3000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_3000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The blue lines in each graph represents the values we logged during the extra 1000 epochs. The loss is getting smaller and the predictions for both companies are improving! Hooray!!! Because it looks like there is even more room for improvement, let's add 2000 more epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb14b21-2aa4-4d37-a58c-d2fb789c06c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_1/checkpoints/epoch=2999-step=6000.ckpt\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:346: UserWarning: The dirpath has changed from '/Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_1/checkpoints' to '/Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_1/checkpoints/epoch=2999-step=6000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new trainer will start where the last left off, and the check point data is here: /Users/joshstarmer/Google Drive/stat_quests/jupyter_notebooks_python_grid/lstm_demo/lightning_logs/version_1/checkpoints/epoch=2999-step=6000.ckpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1555: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2801ba3524c4e43a536961dca40ed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 2it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "## The logs suggest that maybe more training might help.\n",
    "## Maybe adding 2000 more epochs will improve the model a little bit more.\n",
    "\n",
    "## In order to pick up from where we left off, we have to tell the trainer where the most recent checkpoint files are saved\n",
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\"\n",
    "print(\"The new trainer will start where the last left off, and the check point data is here: \" + \n",
    "      path_to_best_checkpoint + \"\\n\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5000) # Before, max_epochs=3000, so, by setting it to 5000, we're adding 2000 more.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231699e4-56eb-4c1b-b48c-e146b8d424fd",
   "metadata": {},
   "source": [
    "Now that we have added 2000 more epochs to the training (for a total of 5000 epochs), let's check the predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c653dc-54ed-4c1a-aca0-419e2311f75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor(0.0022)\n",
      "Company B: Observed = 1, Predicted = tensor(0.9693)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4b82e-6635-4dcd-b889-8374ecb93dad",
   "metadata": {},
   "source": [
    "...and they look good!!! The prediction for Company A is super close to 0 and the prediction for Company B is close to 1. Now let's look at the graphs in TensorBoard by reloading that page.\n",
    "\n",
    "<img src=\"./images/train_loss_5000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_5000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_5000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
    "\n",
    "The dark red lines show how things changed when we added an additional 2000 epochs to the training, for a total of 5000 epochs. Now we see that the loss (train_loss) and the predictions apper to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "Lastly, let's print out the final estimates for the Weights and Biases. In theory, they should be the same (within rounding error) as what I used in the StatQuest on Long Short-Term Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f4e6b4-7f55-4946-8e76-7a52cf2a39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "wf1 tensor(2.7043)\n",
      "wf2 tensor(1.6307)\n",
      "bf1 tensor(1.6234)\n",
      "wr1 tensor(1.9983)\n",
      "wr2 tensor(1.6525)\n",
      "br1 tensor(0.6204)\n",
      "wp1 tensor(1.4122)\n",
      "wp2 tensor(0.9393)\n",
      "bp1 tensor(-0.3217)\n",
      "wo1 tensor(4.3848)\n",
      "wo2 tensor(-0.1943)\n",
      "bo1 tensor(0.5935)\n"
     ]
    }
   ],
   "source": [
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1dbfc-ba9b-4eb3-9906-71a0edebc43d",
   "metadata": {},
   "source": [
    "## DOUBLE BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff877386-0b56-4c60-bcd1-bfa123b9fe7f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be30faf-c262-448a-b60d-8455ff9af997",
   "metadata": {},
   "source": [
    "<a id=\"train_nnLSTM\"></a>\n",
    "# Using and optimzing the PyTorch LSTM, nn.LSTM()\n",
    "\n",
    "Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let's learn how to take advantage of PyTorch's `nn.LSTM()` function. For the most part, using `nn.LSTM()` allows us to simplify the `__init__()` function and the `forward()` function. The other big difference is that this time, we're not going to try and recreate the parameter values we used in the StatQuest on Long Short-Term Memory, and that means we can set the learning rate for the Adam to **0.1**. This will speed up training a lot. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the weights and biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "        \n",
    "        self.state = 0 # this keeps track of which output we are trying to predict for logging\n",
    " \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector    \n",
    "        input_trans = input.view(len(input), 1, -1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ## logging...\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if (self.state == 0):\n",
    "            self.state = 1\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.state = 0\n",
    "            self.log(\"out_1\", output_i)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d271fad-abd9-4a26-a73b-0f5d401f25bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([[0.6675]])\n",
      "Company B: Observed = 1, Predicted = tensor([[0.6665]])\n"
     ]
    }
   ],
   "source": [
    "model_lstm = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model_lstm.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model_lstm(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model_lstm(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/joshstarmer/opt/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03b0736f298460e96baac90630d0c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After optimization, the parameters are...\n",
      "wf1 tensor(2.7043)\n",
      "wf2 tensor(1.6307)\n",
      "bf1 tensor(1.6234)\n",
      "wr1 tensor(1.9983)\n",
      "wr2 tensor(1.6525)\n",
      "br1 tensor(0.6204)\n",
      "wp1 tensor(1.4122)\n",
      "wp2 tensor(0.9393)\n",
      "bp1 tensor(-0.3217)\n",
      "wo1 tensor(4.3848)\n",
      "wo2 tensor(-0.1943)\n",
      "bo1 tensor(0.5935)\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
    "\n",
    "# trainer = L.Trainer(max_steps=600)\n",
    "trainer.fit(model_lstm, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([[6.8010e-05]])\n",
      "Company B: Observed = 1, Predicted = tensor([[0.9809]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model_lstm(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model_lstm(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3",
   "metadata": {},
   "source": [
    "And, as we can see, after just 300 epochs, the LSTM is making great predictions.\n",
    "\n",
    "Lastly, let's refresh the TensorBoard page to see the latest graphs. **NOTE:** To make it easier to see what we just did, deselect `version_0`, `version_1` and `version_2` and make sure `version_3` is checked on the left-hand side of the page, under where it says `Runs`. See below. This allows us to just look at the log files from the most rescent training, which only went for 300 epochs.\n",
    "\n",
    "<img src=\"./images/selecting_run_version_3.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n",
    "<img src=\"./images/train_loss_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_0_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_1_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f184fa-befe-4c9e-a7f5-e718ab84ea6d",
   "metadata": {},
   "source": [
    "In all three graphs, the loss (train_loss) and the predictions for Company A (out_0) and Company B (out_1) started to taper off after 500 steps, or just 250 epochs, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
